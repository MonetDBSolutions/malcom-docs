\section{Introduction}
\label{Introduction}
Since its inception, database designers have keenly looked at the opportunities to use large,
distributed processing platforms. Cluster-based products are readily available \cite{},
but are often limited to a few tens of compute nodes and call for a strong engineering
to avoid hardware bottlenecks, such as in Exadata \cite{}, SQL PWD \cite{}, IBM Blu \cite{} and ....

A plethora of research activities have showen that in all but the simplest cases,
achieving a good performance is at least very hard. Especially when a query
involves joins spread over multiple compute nodes and requires expensive data exchanges.

The predominant way out, taken in NoSQL systems  \cite{Casandra,Impala}
is to address part of the problem space by focusing on select-aggregate queries.
This focus on part of the problem space for distributed query processing has
been pivotal to support big-data analytics in many real-world circumstances,
as shown by the rapidly growing popularity of Spark \cite{},
which is widely used nowadays to encode distributed applications.
The basic abstraction in Spark is a Resilient Distributed Dataset (RDD),
which represents an immutable partitioned collection of elements that can be
operated on in parallel using operators, such as map, filter, persist and aggregates.
Moreover, the RDD is the basic component to exchange between
operators, threads, cores and machines.
In essence, an RDD can be seen as a relational table used for interoperability,
an approach that can be traced back to Microsoft's ODE \cite{} used for decades
to exchange data between DBMS and applications. Similar functional abstractions
can nowadays be found in R's Dataframes \cite{} and Python Pandas\cite{}.

%explain resident/resilient, the former tells it can be re-constructed upon need.

Although in most cases, it is easier to scale-up for improved response time,
partitioning a database to benefit from the low price tag, better use of parallel IO,
or resource limitations of smaller machines is still worth considering.
Distributed database management systems have followed the same route \cite{distributed DB}.
However, the major hurdle that has blocked the progress for decades is the insurmountable task
to create an optimizer to derived an optimal plan \cite{}.

In this paper we take a fresh look at optimizing query processing in the context
of where intermediates in a query plan are fully materialized before passing on
towards the next operator. This model fits the Spark programming model,
but also the query execution model underlying MonetDB.
Resilient intermediates provides new avenues for query optimization and
scheduling as its underlying computation model is based on
materialization of all intermediate steps.

The main contributions of this paper are
\begin{itemize}
	\item we develop a simulator to predict size and processing bounds for queries based on resident intermediates.
	\item We provide a novel optimizer to minimize memory footprint and processing time
	\item We demonstrate the approach is robust against varying data distributions.
	\item We demonstrate the quality of our approach using an extensive evaluation against TPC-H and real-world databases.
	\item ...
\end{itemize}

The approach taken differs from traditional Cost-Based Optimizers (CBO), which generally sample the state of actual execution of primitive operations, e.g. Oracle CBO\footnote{\url{https://docs.oracle.com/cd/B10500_01/server.920/a96533/optimops.htm}} and HIVE CBO\footnote{\url{https://hortonworks.com/blog/hive-0-14-cost-based-optimizer-cbo-technical-overview/}}, because after each query step we have precise knowledge of the resources claimed.
Malcom harvests this information to predict future operations of similar nature.
The rational stems from the common knowledge that many database application
environments have a limited number of ``business transactions'' or ``BI templates'' where only some parameters are changed with each call.
This knowledge has been used in the past to, for instance, drive development of DBA wizards\footnote{E.g. MicroSoft AutoAdmin: \url{https://www.microsoft.com/en-us/research/project/autoadmin/}} for index selection and self-tuning optimizers \footnote{E.g. IBM DatArces Optimizer: \url{https://www.ibm.com/us-en/marketplace/datarcs-optimizer}} to avoid expensive join paths.

Outline of paper. Section \ref{Background} provides a short introduction
to the MonetDB architecture and the projects involved.

\subsection{Use Cases}
\begin{enumerate}
  \item Prediction of the memory footprint
  \item Instruction ordering by the compiler
  \item Parallelism level
  \item Join dictionary build
\end{enumerate}
