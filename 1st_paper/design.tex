\section{Micro Models}
With an abundance of events we can start to derive models for each of the instructions.
A micro model is derived that can be used later for ease of simulation
alternative execution plans. We study them based on similar properties.

\subsection{Argument size}
When we try to predict an instruction, unless it is the first one,
we do not know exactly the argument size. So starting from the beginning,
we build a tree associating each variable to a prediction of the instruction it was produced,
so that we can use the predicted return size as input to the next instruction
we want to predict.

\subsection{Arithmetic Operations}
The operations can be grouped by complexity to predict the outcome of a hypothetical operation.
The first group includes the {\em batcalc} operations.
They all obey the following structure:
\begin{verbatim}
  res:bat[:lng]:= batcalc.==( l:bat[:lng],r:bat[:lng])
  res:bat[:lng]:= batcalc.==( l:bat[:lng],r::lng)
  res:bat[:lng]:= batcalc.==( l:lng,r:bat[:lng])
  res:bat[:lng]:= batcalc.==( l:lng,r:lng)

\end{verbatim}
The argument is either scalar or a reference to a column.
However, in all cases the number of result tuples can be looked up from the argument events.
The processing time depends mostly on the type's footprint and the influence of concurrent activity.


\subsection{Limit Operations}
Limit and sample operators(firstn, and sample? in MAL) are also trivial,
we just output the minimum of the argumens size and the limit size.


\subsection{Aggregate Operations}
This category includes operations like sum, avg, min, max, single, dec\_round
The count of the result is obviously one.

\subsection{Select Operator}
\subsubsection{Range Selects}
To predict the size of a range select, we used the kNN algorithm,
using k=5, to find the 5 closest selects to the one we want to predict,
considering the lower and higher bounds as distance metrics.
When we are facing a one bound select ($<$,$>$ etc) we use the
dataset statistics to fill the other bound (e.g in case of $<$ we want the column min).
\subsubsection{Point Selects}
Again we use the kNN to find the 5 closest points,
and extrapolate based on the argument size.

\subsubsection{Like Selects}
Future work :P

\subsubsection{Subsequent Selects}
%TODO make it more general
In case of intermediate select instructions, the ranges are not adequate to
make an accurate prediction, because the argument size may vary based on the
previous selects. To overcomes this we incorporated the estimations of the
previous intructions, to build a graph that relates each variable to a size
estimation. This way we cal also use the argument estimation to predict the
size of the select result. The code for selection prediction is shown at
Figure\ref{sel:code}. In real life it is very common to find correlated columns,
in which the first selection may affect non-linearly the second.
In our design we do not handle such cases(Future work).

\begin{figure}[t]
\begin{lstlisting}[frame=single]
  def div(i1, i2):
    return (i1.hi-i1.lo) / (i2.hi-i2.lo)

  def extrapolate(traini, testi):
      traini.cnt*div(testi,traini)*testi.approx_arg_cnt/traini.argcnt

  def predict(testi, traind, approxG):
    knn5 = traind.knn(testi,5)
    return sum([i.extrapolate(testi) for i in knn5]) / len(knn5)
\end{lstlisting}
  \caption{Code snippet for making predictions for range selects}
  \label{sel:code}
\end{figure}


\subsection{Join}
(Run a kNN(k=5) on the joins we have in our dictionary,
that operate on the same two columns if possible,
and find the ones closest based on the argument sizes).

\subsection{Grouping Operations}
The groupby signature is:
\begin{verbatim}
  (bat[:oid], bat[:oid], bat[:X]) := group.groupdone(bat[:X]);
\end{verbatim}
The first return value is the ids of the grouped column, the second is
the ids of the discinct values of the column(the third what ??). To predict
the size of the return variables, we use the ground statistics of the column
(size, count, discinct values).

Subgroupdone has a similar signature to groupdone, but it operates on intermmediate
variables, where we do not have the column information. In this case, we run a
kNN based on the argument sizes.

Orderby is translated into sort in MAL.
\begin{verbatim}
(bat[:X],bat[:oid],bat[:oid]) := algebra.sort(bat[:X], bat[:oid], bat[:oid], bit, bit)
\end{verbatim}
The size of each return value is exactly the same as the corresponding argument size.

\subsection{Set instructions}
We do not have any relevant information to do an accurate guess for the
set instructions, so we output the worst case of the result.
$$ intersect(A,B) ~= min(A,B)$$
$$ merge(A,B) ~= A+B $$
$$ diff(A,B) ~= A $$
% diff,union,intersect

\subsection{Load instructions}
\subsubsection{bind}
Bind instruction loads(or memory maps) a column into memory,
thus the result size is the size of the column.
\subsubsection{bind\_idxbat, tid}
Tid and bind\_idxbat do not produce any memory overhead

\subsection{Special cases}
\subsubsection{String similarity}
To predict a selection of a string column,
we used the levenshtein distance as a metric...
\subsubsection{Projection returning str}
In some weird cases, the projection instruction
returns str(copies from heap) instead of ids,
which makes hard to predict. In this case, we run a kNN
based on the type and argument distance, to find a similar
instruction use this to find the output size.
